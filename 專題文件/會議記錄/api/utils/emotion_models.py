# backend/api/utils/emotion_models.py
from __future__ import annotations
from typing import List, Tuple, Dict
import os
import re
import jieba
import torch
import requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) è¨­å®šèˆ‡è¼‰å…¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

MODEL_NAME = "IDEA-CCNL/Erlangshen-RoBERTa-110M-Sentiment"

# æ¬Šé‡ï¼šæ¨¡å‹ vs è©å…¸ï¼ˆå¯å¾®èª¿ï¼‰
W_MODEL = 0.6
W_LEX = 0.4
# ä¸­ç«‹é–€æª»ï¼ˆ-T ~ +T æ˜¯ neutralï¼‰
T_NEU = 0.15

# æœ€å¤§ token é•·åº¦ï¼ˆåˆ†æ®µæ¨è«–å°±ä¸ç”¨å¾ˆå¤§ï¼‰
MAX_LEN = 256
# é•·æ–‡æ¯æ®µå­—æ•¸ï¼ˆé tokenï¼Œæ˜¯å­—/å­—å…ƒç²—ä¼°ï¼‰
CHUNK_CHAR = 220

# å®‰å…¨è¼‰å…¥æ¨¡å‹ï¼ˆè¼‰å…¥å¤±æ•—ä¸é˜»æ–·æ•´é«”åŠŸèƒ½ï¼‰
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
    model.eval()
except Exception as e:
    print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
    tokenizer = None
    model = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) è©å…¸ã€å¦å®šè©ã€å¼·åº¦è©ã€emojiã€äº‹ä»¶è©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
POSITIVE_WORDS = set([
    "é–‹å¿ƒ","å¿«æ¨‚","æ»¿è¶³","æœŸå¾…","èˆˆå¥®","å¹¸ç¦","æ”¾é¬†","è¸å¯¦","å®‰å¿ƒ","é †åˆ©",
    "çˆ½","è¶…çˆ½","chill","é–‹è–°","æœ‰æˆå°±æ„Ÿ","èˆ’æœ","æ”¾å¿ƒ","é›€èº","å¾ˆæ£’","å¤ªå¥½äº†",
    "æ„‰å¿«","å–œæ‚…","é«˜èˆˆ","å¿«æ´»","æŒ¯å¥®","é©šå–œ","éç™®","ç†±è¡€","å–œæ¨‚","æ¬£å–œè‹¥ç‹‚",
    "å¾ˆå–œæ­¡","å¾ˆé–‹å¿ƒ","å¾ˆæ»¿","è‡ªç”±","è‡ªåœ¨","è¼•é¬†","å¯§éœ","å®‰ç©©","è¢«è‚¯å®š","è¢«æ¬£è³",
])

NEGATIVE_WORDS = set([
    "æ‚²å‚·","ç„¦æ…®","æ†¤æ€’","ç…©èº","å´©æ½°","ç„¡åŠ›","ç—›è‹¦","ç´¯","å­ä¸–","çˆ†ç‚¸","å¾ˆç…©",
    "å¤±æœ›","æ²®å–ª","é›£é","ä½è½","ä¸çˆ½","çˆ›é€","å¿ƒç´¯","å¾ˆæ‚¶","å£“åŠ›å±±å¤§","å–ªæ°£",
    "å¯æƒœ","å¤±è½","æ†‚é¬±","ææ‡¼","å®³æ€•","ç·Šå¼µ","æ…Œå¼µ","ä¸å®‰","æ“”å¿ƒ","æ··äº‚","ç³Ÿç³•",
    "è¢«å¿½è¦–","è¢«è²¬å‚™","è¢«å˜²ç¬‘","ç¾æ„§","ä¸Ÿè‡‰","ç„¡åŠ©","ç„¡å¥ˆ","ç„¡èƒ½ç‚ºåŠ›","æŠ“ç‹‚",
])

# äº‹ä»¶è©ï¼ˆä¸æ˜¯å¼·æƒ…ç·’è©ï¼Œä½†çµ¦ã€Œæ–¹å‘æ€§åŠ åˆ†/æ‰£åˆ†ã€ï¼Œä¾‹å¦‚ï¼šå‹•ç‰©åœ’/ç”Ÿæ—¥åæ­£å‘ï¼‰
POSITIVE_EVENTS = set([
    "å‹•ç‰©åœ’","ç”Ÿæ—¥","ç´„æœƒ","æ—…è¡Œ","æ—…éŠ","å‡ºéŠ","éŸ³æ¨‚æœƒ","æ¼”å”±æœƒ","æ…¶ç”Ÿ","ç•¢æ¥­å…¸ç¦®","å‡è·","æ”¾å‡",
])

NEGATIVE_EVENTS = set([
    "è‘¬ç¦®","ç”Ÿç—…","å¤±æ¥­","åˆ†æ‰‹","åµæ¶","è¢«ç½µ","è€ƒè©¦å¤±æ•—","è»Šç¦","éä¸–","å»ä¸–",
])

# å¦å®šè©ï¼ˆç¿»è½‰æ¥µæ€§ï¼Œä½œç”¨ç¯„åœï¼šå‰ä¸€å€‹ tokenï¼‰
NEGATIONS = set(["ä¸","æ²’","æ²’æœ‰","åˆ¥","ç„¡","æœª","ä¸å¤ª","ä¸å¤§","ä¸æ€éº¼"])
# å¼·åº¦è©ï¼ˆåŠ æ¬Šï¼‰
STRONG_INTENSIFIERS = set(["è¶…","è¶…ç´š","éå¸¸","çˆ†","çˆ†ç‚¸","å·¨","è¶…ç´šç„¡æ•µ","è¶…ç´šçˆ†"])
WEAK_INTENSIFIERS = set(["å¾ˆ","æŒº","è »","æœ‰é»","ç¨å¾®","æœ‰äº›"])

# å¸¸è¦‹ emoji çš„æ¥µæ€§
EMOJI_POLARITY = {
    "ğŸ˜€": 2, "ğŸ˜„": 2, "ğŸ˜": 2, "ğŸ¤£": 2, "ğŸ˜‚": 2, "ğŸ˜Š": 1, "ğŸ™‚": 1, "ğŸ˜": 2, "ğŸ‘": 1, "âœ¨": 1,
    "ğŸ˜": 0, "ğŸ¤”": 0, "ğŸ˜¶": 0,
    "ğŸ˜•": -1, "ğŸ˜": -2, "ğŸ˜Ÿ": -2, "ğŸ˜¢": -2, "ğŸ˜­": -3, "ğŸ˜¡": -3, "ğŸ¤¬": -3, "ğŸ‘": -1, "ğŸ’”": -2,
}

# ä¸»é¡Œåµæ¸¬é—œéµè©
AWARENESS_WORDS = ["æ³¨æ„åˆ°","è¦ºå¾—","ç™¼ç¾","æ„è­˜åˆ°","é«”æœƒ","å¯Ÿè¦º"]
CONTROL_WORDS   = ["å¿ä½","å£“ä¸‹","æ§åˆ¶","å†·éœ","ä¸ç™¼ç«","å£“æŠ‘","å…‹åˆ¶"]
MANAGEMENT_WORDS= ["å®‰æ’","è¦åŠƒ","è™•ç†","é¢å°","èª¿æ•´","æ‡‰å°","æ‡‰ä»˜","è§£æ±º","å®Œæˆ"]

# é è¨­è¨Šæ¯ï¼ˆä¸ç”¨å‘¼å¸å£ä»¤ï¼‰
DEFAULT_MSG = {
    "positive": "è½èµ·ä¾†ä½ çš„å¿ƒæƒ…å¾ˆå¥½ï¼Œä¿æŒé€™ä»½èƒ½é‡å»é¢å°æ¥ä¸‹ä¾†çš„äº‹æƒ…å§ï¼",
    "neutral" : "ä½ æŠŠç•¶ä¸‹çš„ç‹€æ…‹è¨˜éŒ„ä¸‹ä¾†äº†ï¼Œé€™æœ¬èº«å°±æ˜¯å¾ˆå¥½çš„ç·´ç¿’ã€‚",
    "negative": "è®€èµ·ä¾†ä»Šå¤©æœ‰é»ä¸å®¹æ˜“ï¼Œé¡˜ä½ è¢«å¥½å¥½çœ‹è¦‹ï¼Œä¹Ÿèƒ½å°è‡ªå·±æº«æŸ”ä¸€äº›ã€‚",
}

# Geminiï¼ˆå¯æœ‰å¯ç„¡ï¼›æ²’æœ‰ API key å°±ç”¨ DEFAULT_MSGï¼‰
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) å·¥å…·ï¼šé•·æ–‡åˆ†æ®µ / Jieba + emoji token åŒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_EMOJI_RE = re.compile(
    "["               # é€™å€‹æ­£å‰‡èƒ½æŠ“å¤§éƒ¨åˆ† emojiï¼ˆä¸æ˜¯å®Œç¾ï¼Œä½†è¶³ä»¥ï¼‰
    "\U0001F300-\U0001F6FF"
    "\U0001F900-\U0001F9FF"
    "\U0001F1E6-\U0001F1FF"
    "\u2600-\u26FF"
    "\u2700-\u27BF"
    "]+")

def _split_chunks_by_char(s: str, chunk_len: int = CHUNK_CHAR) -> List[str]:
    s = s.strip()
    if not s:
        return [s]
    return [s[i:i+chunk_len] for i in range(0, len(s), chunk_len)]

def _tokenize_with_emoji(text: str) -> List[str]:
    # å…ˆæŠŠ emoji ç”¨ç©ºç™½åˆ†é–‹ï¼Œå†ä¸Ÿ jieba
    text = _EMOJI_RE.sub(lambda m: f" {m.group(0)} ", text)
    # ä¹ŸæŠŠæ¨™é»é©åº¦åˆ‡é–‹ï¼ˆç°¡å–®è™•ç†ï¼‰
    text = re.sub(r"([!?ï¼ï¼Ÿã€‚ï¼Œ,.â€¦~])", r" \1 ", text)
    tokens = list(jieba.cut(text))
    # ç§»é™¤ç©ºç™½ token
    return [t.strip() for t in tokens if t.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) è©å…¸åˆ†æ•¸ï¼šå¦å®š/å¼·åº¦/emoji/é©šå˜†è™Ÿ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _lexicon_score(tokens: List[str]) -> float:
    score = 0.0
    hits = 0

    for i, w in enumerate(tokens):
        base = 0.0
        if w in POSITIVE_WORDS:
            base = 1.0
        elif w in NEGATIVE_WORDS:
            base = -1.0
        elif w in EMOJI_POLARITY:
            base = float(EMOJI_POLARITY[w])
        else:
            continue

        # ä¿®é£¾è©ï¼šçœ‹å‰ä¸€å€‹ token
        modifier = 1.0
        prev = tokens[i-1] if i > 0 else ""
        if prev in NEGATIONS:
            base *= -1.0
        if prev in STRONG_INTENSIFIERS:
            modifier *= 1.5
        elif prev in WEAK_INTENSIFIERS:
            modifier *= 1.2

        score += base * modifier
        hits += 1

    # é©šå˜†è™Ÿå¼·åº¦ï¼ˆç²—ç•¥ 0.1/å€‹ï¼Œæœ€å¤šåŠ åˆ° 0.3ï¼‰
    exclam = sum(1 for t in tokens if t in ["!","ï¼","!!!","ï¼ï¼"])
    score += min(0.3, exclam * 0.1)

    if hits == 0:
        return 0.0
    # æ­£è¦åŒ–åˆ° [-1, 1]
    norm = score / (hits * 1.5)
    return max(-1.0, min(1.0, norm))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) æ¨¡å‹åˆ†æ•¸ï¼ˆ2/3 é¡è‡ªå‹•é©é…ï¼‰ï¼Œé•·æ–‡å–å¹³å‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@torch.no_grad()
def _model_score(text: str) -> Tuple[float, float]:
    """
    å›å‚³ï¼š(model_sentiment_score in [-1,1], avg_confidence)
    - è‹¥ num_labels == 3ï¼šè¦–ç‚º (neg, neu, pos)ï¼Œscore = p_pos - p_neg
    - è‹¥ num_labels == 2ï¼šè¦–ç‚º (neg, pos)ï¼Œ   score = p_pos - p_neg
    - å…¶ä»–æƒ…æ³ï¼šå›å‚³ (0, 0)
    """
    if not tokenizer or not model:
        return 0.0, 0.0

    chunks = _split_chunks_by_char(text, CHUNK_CHAR)
    scores, confs = [], []

    for chunk in chunks:
        if not chunk.strip():
            continue
        try:
            inputs = tokenizer(
                chunk, return_tensors="pt",
                truncation=True, padding=True, max_length=MAX_LEN
            )
            logits = model(**inputs).logits.squeeze(0)  # (num_labels,)
            num_labels = logits.shape[-1]
            probs = torch.softmax(logits, dim=-1)

            if num_labels == 3:
                p_neg, p_neu, p_pos = probs[0].item(), probs[1].item(), probs[2].item()
                score = p_pos - p_neg
                conf = max(p_neg, p_neu, p_pos)
            elif num_labels == 2:
                p_neg, p_pos = probs[0].item(), probs[1].item()
                score = p_pos - p_neg
                conf = max(p_neg, p_pos)
            else:
                continue

            scores.append(float(score))
            confs.append(float(conf))
        except Exception:
            # æŸæ®µå¤±æ•—å°±ç•¥éï¼Œä¸è®“æ•´é«”å£æ‰
            continue

    if not scores:
        return 0.0, 0.0
    return float(sum(scores)/len(scores)), float(sum(confs)/len(confs))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) Gemini è¨Šæ¯ï¼ˆå¯ç„¡ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _generate_gemini_message(summary: str):
    if not GEMINI_API_KEY:
        return None
    try:
        headers = {"Content-Type": "application/json", "X-goog-api-key": GEMINI_API_KEY}
        payload = {
            "contents": [{
                "parts": [{
                    "text": (
                        "è«‹ä»¥çœŸèª ã€è‡ªç„¶ã€ä¸èª‡å¼µçš„èªæ°£ï¼Œå°ä¸‹åˆ—æƒ…ç·’æ‘˜è¦å¯« 1ï½2 å¥æ”¯æŒè¨Šæ¯ã€‚"
                        "é¿å…ã€åšå¹¾æ¬¡å‘¼å¸ã€ç­‰æŒ‡ä»¤ã€å¤šç”¨åŒç†èˆ‡é¼“å‹µï¼›æ­£å‘å°±æé†’ä¿æŒèƒ½é‡ï¼Œ"
                        "è² å‘å°±æº«æŸ”æ¥ä½èˆ‡çµ¦ä¸€å€‹å¯è¡Œçš„å°æ­¥é©Ÿï¼ˆå¦‚ï¼šå¯«ä¸‰ä»¶å°ç¢ºå¹¸ã€å‚³è¨Šæ¯çµ¦ä¿¡ä»»çš„äººï¼‰ã€‚"
                        "ä¸è¦ä½¿ç”¨åå•å¥ï¼Œä¹Ÿä¸è¦æµæ°´å¸³ã€‚\n"
                        f"æƒ…ç·’æ‘˜è¦ï¼š{summary}"
                    )
                }]}
            ]}
        r = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=12)
        r.raise_for_status()
        data = r.json()
        cands = data.get("candidates") or []
        if not cands:
            return None
        content = (cands[0] or {}).get("content") or {}
        parts = content.get("parts") or []
        if not parts:
            return None
        text = (parts[0] or {}).get("text")
        return text.strip() if isinstance(text, str) and text.strip() else None
    except Exception as e:
        print("âŒ Gemini API éŒ¯èª¤ï¼š", e)
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) è¦å‰‡å¼æç¤ºèªï¼ˆç„¡ Gemini æ™‚ç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _compose_support_message(label: str, keywords: List[str], topics: List[str], extra_hint: str | None) -> str:
    topic_hint = ""
    if topics:
        topic_hint = f"ï¼ˆä½ ä¹Ÿå±•ç¾äº†{ 'ã€'.join(topics) }ï¼‰"

    if label == "positive":
        base = "è½èµ·ä¾†ä½ çš„å¿ƒæƒ…å¾ˆä¸éŒ¯ï¼Œä¿æŒé€™ä»½å¥½èƒ½é‡å»é¢å°æ¥ä¸‹ä¾†çš„äº‹æƒ…å§ï¼"
        if extra_hint:
            base = f"è½èµ·ä¾†ä½ çš„å¿ƒæƒ…å¾ˆä¸éŒ¯ï¼Œ{extra_hint}ï¼ŒæŠŠé€™ä»½å¥½èƒ½é‡å¸¶è‘—èµ°å§ï¼"
        return base + ("" if not topic_hint else f"{topic_hint}")
    elif label == "negative":
        base = "è®€èµ·ä¾†ä»Šå¤©ä¸å¤ªå®¹æ˜“ï¼Œæ„Ÿè¬ä½ å¯«ä¸‹ä¾†ï¼Œé€™å·²ç¶“å¾ˆå‹‡æ•¢ã€‚é¡˜ä½ è¢«å¥½å¥½çœ‹è¦‹ï¼Œä¹Ÿå°è‡ªå·±æº«æŸ”ä¸€äº›ã€‚"
        return base + ("" if not topic_hint else f"{topic_hint}")
    else:
        base = "ä½ æ¸…æ¥šåœ°è¨˜éŒ„äº†æ­¤åˆ»ï¼Œé€™æœ¬èº«å°±æ˜¯å¾ˆå¥½çš„ç·´ç¿’ã€‚"
        if extra_hint:
            base = f"{base} {extra_hint}"
        return base + ("" if not topic_hint else f"{topic_hint}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) ä¸»æµç¨‹ï¼šæ¨¡å‹+è©å…¸ èåˆã€äº‹ä»¶åŠ æ¬Šã€ä¸»é¡Œæ¨™è¨»ã€è¨Šæ¯ç”¢ç”Ÿ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_sentiment(text: str):
    """
    å›å‚³ï¼š(label, message, keywords, topics)
    label âˆˆ {'positive','neutral','negative'}
    """
    raw = (text or "").strip()
    if not raw:
        return "neutral", DEFAULT_MSG["neutral"], [], []

    tokens = _tokenize_with_emoji(raw)

    # è©å…¸åˆ†æ•¸
    lex_s = _lexicon_score(tokens)

    # æ¨¡å‹åˆ†æ•¸ & ä¿¡å¿ƒ
    mdl_s, mdl_conf = _model_score(raw)

    # æ¬Šé‡èª¿æ•´ï¼šæ¨¡å‹æ²’è¼‰åˆ°/ä¿¡å¿ƒä½ â†’ æé«˜è©å…¸æ¯”é‡
    if mdl_conf == 0.0:        # æ²’æ¨¡å‹æˆ–å®Œå…¨å¤±æ•—
        w_model, w_lex = 0.0, 1.0
    elif mdl_conf < 0.55:      # ä¿¡å¿ƒåä½ â†’ 50/50
        w_model, w_lex = 0.5, 0.5
    else:
        w_model, w_lex = W_MODEL, W_LEX

    # äº‹ä»¶åŠ æ¬Šï¼ˆå¼±åŠ åˆ†/æ‰£åˆ†ï¼‰
    event_bonus = 0.0
    pos_events_hit = [e for e in POSITIVE_EVENTS if e in raw]
    neg_events_hit = [e for e in NEGATIVE_EVENTS if e in raw]
    if pos_events_hit:
        event_bonus += 0.15   # ä¾‹å¦‚ï¼šæåˆ°ã€Œå‹•ç‰©åœ’/ç”Ÿæ—¥ã€â†’ åæ­£å‘
    if neg_events_hit:
        event_bonus -= 0.15

    # ç‰¹æ®Šå£èªå¼·åŒ–ï¼šå‡ºç¾ã€Œèˆˆå¥®ã€æœŸå¾…ã€ç­‰å¼·æ­£å‘è©
    if any(w in raw for w in ["èˆˆå¥®","è¶…èˆˆå¥®","è¶…æœŸå¾…","è¶…ç´šæœŸå¾…"]):
        event_bonus += 0.15

    # æœ€çµ‚åˆ†æ•¸
    final_score = w_model * mdl_s + w_lex * lex_s + event_bonus
    final_score = max(-1.0, min(1.0, final_score))

    if final_score > T_NEU:
        label = "positive"
    elif final_score < -T_NEU:
        label = "negative"
    else:
        label = "neutral"

    # é—œéµè©èˆ‡ä¸»é¡Œï¼ˆå¯å†æ“´å……ï¼‰
    keywords = [w for w in tokens if (w in POSITIVE_WORDS or w in NEGATIVE_WORDS or w in EMOJI_POLARITY)]
    # æŠŠäº‹ä»¶ä¹Ÿæ”¾é€² keywords æ–¹ä¾¿å‰ç«¯å±•ç¤º
    keywords += [e for e in pos_events_hit + neg_events_hit if e not in keywords]

    topics = []
    if any(w in tokens for w in AWARENESS_WORDS):
        topics.append("è‡ªæˆ‘è¦ºå¯Ÿ")
    if any(w in tokens for w in CONTROL_WORDS):
        topics.append("è‡ªæˆ‘æ§åˆ¶")
    if any(w in tokens for w in MANAGEMENT_WORDS):
        topics.append("è‡ªæˆ‘ç®¡ç†")

    # æ‘˜è¦çµ¦ Geminiï¼ˆå«ä¸€äº›åµæ¸¬ç·šç´¢ï¼‰
    summary_parts = [
        f"æ•´é«”æ¥µæ€§åˆ†æ•¸ï¼š{final_score:.2f}ï¼ˆæ¨¡å‹ {mdl_s:.2f}Ã—{w_model} + è©å…¸ {lex_s:.2f}Ã—{w_lex}ï¼›äº‹ä»¶åŠ æ¬Š {event_bonus:+.2f}ï¼›ä¿¡å¿ƒ {mdl_conf:.2f}ï¼‰",
        f"åˆ¤å®šç‚ºï¼š{label}",
    ]
    if keywords:
        ks = [k for k in keywords if k not in ["ï¼Œ","ã€‚","!","ï¼"]][:8]  # åªå±•ç¤ºå‰ 8 å€‹
        if ks:
            summary_parts.append("ç·šç´¢ï¼š" + ", ".join(ks))
    if topics:
        summary_parts.append("ä¸»é¡Œï¼š" + ", ".join(topics))
    summary_text = "ï¼›".join(summary_parts)

    # å…ˆå˜—è©¦ Geminiï¼Œå¤±æ•—å°±ç”¨è¦å‰‡è¨Šæ¯ï¼ˆä¸è¬›å‘¼å¸ï¼‰
    gem = _generate_gemini_message(summary_text)
    if gem and gem.strip():
        message = gem.strip()
    else:
        # å®¢è£½ä¸€é»çš„å°æç¤ºï¼ˆä¾‹å¦‚åµæ¸¬åˆ°æ­£å‘äº‹ä»¶ï¼‰
        extra_hint = None
        if pos_events_hit and label == "positive":
            extra_hint = f"çœ‹èµ·ä¾†ä½ å°ã€Œ{pos_events_hit[0]}ã€å¾ˆæœŸå¾…"
        message = _compose_support_message(label, keywords, topics, extra_hint)

    return label, message, keywords, topics
